{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0a7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742038d",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822b3ba",
   "metadata": {},
   "source": [
    "*Disclaimer*\n",
    "\n",
    "### I am not an expert. I don't really know that much about reinforcement learning as applied to a machine learning context beyond a couple of books and some research. There are probably people in here who know more than me and I welcome any corrections and or critisim or questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3652b8",
   "metadata": {},
   "source": [
    "Tuning sounds way better than fiddling with stuff until it works mo' better\n",
    "\n",
    "\n",
    "### What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning is an area of machine learning that involves:\n",
    "- **Policy**: The strategy an agent employs to determine its actions.üïπÔ∏è\n",
    "- **Value Function**: An estimation of future rewards given a state and action.üéØ\n",
    "- **Model of the Environment**: Optionally used to simulate outcomes of actions (not always used).üìñ\n",
    "\n",
    "A cross between neuroscience, behaviorist psychology, engineering and mathematics.\n",
    "\n",
    "\n",
    "It is not unsupervised learning (find the pattern) or supervised learning (you've trained on the pattern). Fundamentally, this is more about training the actions of an agent, first by trial and error, and then by figuring out how to get more of a good thing.\n",
    "\n",
    "It is also *not* an LLM - but it has been used as the final step in training them!\n",
    "\n",
    "### How Did It Start?\n",
    "\n",
    "Originating from early studies in behavioral psychology, reinforcement learning has evolved through:\n",
    "- Thorndike's Law of Effect, where behaviors followed by rewards become more likely.\n",
    "\n",
    "_When a modifiable connection between a situation and a response is made and is accompanied or followed by a satisfying state of affairs, that connection's strength is increased: When made and accompanied or followed by an annoying state of affairs its strength is decreased. The strenghtening effect of satisfyingness (or the weakening effect of annoyingness) upon a bond varies with the closeness of the connection between it and the bond_\n",
    "\n",
    "\n",
    "### Operant conditioning\n",
    "\n",
    "Note: this is *operant* conditioning, not *classical* conditioning. Mowgli's volunary behaviour is affected the consequences of his behaviour.\n",
    "\n",
    "\n",
    "Let's see my glamourous assistant Mowgli assisting me:\n",
    "\n",
    "![come](come.gif)\n",
    "![shake](shake.gif)\n",
    "![lie_down](lie_down.gif)\n",
    "![jump](jump.gif)\n",
    "\n",
    "We can connect this to our earlier statement. \n",
    "Mowgli's *policy* is a strategy that maximises getting treats.\n",
    "Mowgli's *rewards* are his treats.\n",
    "Mowgli's value function estimates that when he sees me do certain motions combined with hearing certain sound queues, he sits/lies down/comes/shakes because he *thinks* that that will lead to a reward\n",
    "\n",
    "### How can we apply this to machines?\n",
    "\n",
    "Turing talking about how he might train a network:\n",
    "\n",
    "_When a configuation is reached for which the action is undetermined, a random choice for the missing data is made and the appropriate entry is made in the description tentatively, and is applied. When a pain stimulus occurs all tentavie entries are cancelled, and when a pleasure stimulus occurs they are all made permanent_\n",
    "\n",
    "- Skinner's operant conditioning, which introduced the use of consequences for shaping behavior. ‚ö°\n",
    "- Bellman's development of dynamic programming, setting the stage for modern reinforcement learning algorithms.\n",
    "- Sutton and Barto founding the field of reinforcement learning\n",
    "\n",
    "Andrew Barto and Richard Sutton: they began in 1972 and didn't stop publishing papers and researching for 45 years\n",
    "\n",
    "### Cross-discipline advances\n",
    "\n",
    "Advances in psychology were critical to early work in the field of machine learning (and indeed, in it's inception). But what goes around comes around; years of breaking down behavioural whys in models in machine learning was in turn critical to understanding how dopamine works (or in other words, how our value function works).\n",
    "\n",
    "What is Dopamine? \n",
    "\n",
    "Reward? Attention? Novelty? Surprise?\n",
    "\n",
    "It's more explained by expectation - or as they called it in machine learning: expectation.\n",
    "\n",
    "\n",
    "### Temporal Difference Learning\n",
    "\n",
    "As you move forward toward an uncretain future, you maintain a king of \"running expection\" of how promising things seem. In a chess game, it might be the odds you give yourself to win the game. In a video game, it might be how much progress you expect to make or how many points you expect to rack up in total. These guesses fluctuate over time, and in generatil they get more accurate tyhe closer you are to whateve  it is you're trying to predict.\n",
    "\n",
    "As our expectation fluctuates, we get difference between our successive expections, each of which is a learning opportunity. These are temporal differences. \n",
    "\n",
    "Algorithm called TD-lambda\n",
    "\n",
    "\"Learning a guess from a guess\"\n",
    "\n",
    "\n",
    "### Dynamic programming link\n",
    "\n",
    "RL takes the key steps from DP, which are all about making decisions and improving them, and adapts them for situations where you don‚Äôt have all the information from the start. It‚Äôs all about learning the best moves by practicing and seeing the results, much like learning to play a video game better by playing it more and more.\n",
    "\n",
    "\n",
    "### Common Definitions in Reinforcement Learning\n",
    "\n",
    "#### Operant Conditioning vs. Classical Conditioning\n",
    "- **Operant Conditioning**: A form of learning where an individual's behavior is modified by its consequences, such as rewards and punishments.\n",
    "- **Classical Conditioning**: A learning process that occurs when two stimuli are repeatedly paired; a response that is at first elicited by the second stimulus is eventually elicited by the first stimulus alone.\n",
    "\n",
    "#### On-policy vs. Off-policy Learning\n",
    "- **On-policy Learning**: The strategy where the learning agent evaluates and improves the policy that it uses to make decisions.\n",
    "- **Off-policy Learning**: The strategy where the agent learns a potentially different policy from the behavior policy that is used for making decisions and collecting data.\n",
    "\n",
    "#### Common Reinforcement Learning Algorithms\n",
    "- **Proximal Policy Optimization (PPO)**: A policy gradient method for reinforcement learning that uses a clipped surrogate objective function to prevent large policy updates, thereby improving training stability.\n",
    "- **Deep Q-Network (DQN)**: An algorithm that combines Q-learning with deep neural networks to approximate the Q-value function, allowing the agent to learn from high-dimensional sensory input.\n",
    "\n",
    "### Additional Relevant Concepts\n",
    "\n",
    "#### Markov Decision Process (MDP)\n",
    "A mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "\n",
    "#### Temporal Difference (TD) Learning\n",
    "A class of model-free reinforcement learning methods that learn by bootstrapping from the estimated value of subsequent states, rather than waiting for a final outcome.\n",
    "\n",
    "#### Exploration vs. Exploitation\n",
    "A dilemma faced by learning agents about whether to explore new possibilities (exploration) or choose options that are known to yield high rewards (exploitation).\n",
    "\n",
    "#### Reward Function\n",
    "A function used to signal the success of an agent's actions. It's the key feedback signal that guides the learning algorithm in reinforcement learning.\n",
    "\n",
    "#### Œµ-greedy Policy\n",
    "A common policy for controlling the agent's balance between exploration and exploitation. With a certain probability Œµ, the agent explores a random action, and with probability 1-Œµ, it exploits the currently known best action.\n",
    "\n",
    "These definitions and concepts form the foundation of reinforcement learning theory and practice, providing a framework for developing algorithms that enable machines to learn and make decisions autonomously.\n",
    "\n",
    "\n",
    "#### Dynamic Programming\n",
    "This technique solves problems by breaking them into smaller, overlapping subproblems. The results are then stored in a table to be reused so the same problem will not have to be computed again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0495cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63245986\n"
     ]
    }
   ],
   "source": [
    "# Function for nth Fibonacci number (naive, not using dynamic programming)\n",
    " \n",
    "def fibonacci_basic(n):\n",
    "    if n<= 0:\n",
    "        print(\"Incorrect input\")\n",
    "    # First Fibonacci number is 0\n",
    "    elif n == 1:\n",
    "        return 0\n",
    "    # Second Fibonacci number is 1\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci_basic(n-1)+fibonacci_basic(n-2)\n",
    " \n",
    "print(fibonacci_basic(40))\n",
    "\n",
    "### Each number is just re-calculated every single time for every recursive call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457b8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7778742049\n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049]\n"
     ]
    }
   ],
   "source": [
    "# Function for nth fibonacci number - Dynamic Programming\n",
    " \n",
    "FibArray = [0, 1]\n",
    " \n",
    "def fibonacci_dynamic(n):\n",
    "    if n<0:\n",
    "        print(\"Incorrect input\")\n",
    "    elif n<= len(FibArray):\n",
    "        return FibArray[n-1]\n",
    "    else:\n",
    "        temp_fib = fibonacci_dynamic(n-1)+fibonacci_dynamic(n-2)\n",
    "        FibArray.append(temp_fib)\n",
    "        return temp_fib\n",
    " \n",
    "print(fibonacci_dynamic(50))\n",
    "print(FibArray)\n",
    "\n",
    "### Each number is calculated just once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b87a0",
   "metadata": {},
   "source": [
    "### Live Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd5ae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.tenor.com/IJuLSEYNCcAAAAAC/its-happening.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://media.tenor.com/IJuLSEYNCcAAAAAC/its-happening.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79a9017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://gymnasium.farama.org/_images/cart_pole.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bc542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.26.1)\n",
      "Requirement already satisfied: torch>=1.13 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.2)\n",
      "Requirement already satisfied: matplotlib in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.8.1)\n",
      "Requirement already satisfied: opencv-python in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.8.1.78)\n",
      "Requirement already satisfied: pygame in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: psutil in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.66.1)\n",
      "Requirement already satisfied: rich in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (13.6.0)\n",
      "Requirement already satisfied: shimmy~=1.1.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: pillow in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (10.1.0)\n",
      "Requirement already satisfied: autorom~=0.6.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: click in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
      "Requirement already satisfied: requests in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.59.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (6.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
      "Requirement already satisfied: gymnasium in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'stable-baselines3[extra]'\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f2c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym # https://gymnasium.farama.org/\n",
    "import pygame\n",
    "\n",
    "from IPython import display\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "from stable_baselines3.common.evaluation import evaluate_policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fb0e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8630ab",
   "metadata": {},
   "source": [
    "## Cartpole - training an agent to balance a pole on top of a cart.\n",
    "### Action Space: Defines the actions you can take in your environment. So, in our case, we have two actions in our action space:\n",
    " 0: Push cart to the left\n",
    " 1: Push cart to the right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be89a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3e3bf",
   "metadata": {},
   "source": [
    "### Observation Space: Defines what you can see:\n",
    "| Num | Observation           | Min                 | Max                |\n",
    "|-----|-----------------------|---------------------|--------------------|\n",
    "| 0   | Cart Position         | -4.8                | 4.8                |\n",
    "| 1   | Cart Velocity         | -Inf                | Inf                |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24¬∞) | ~ 0.418 rad (-24¬∞) |\n",
    "| 3   | Pole Angular Velocity | -Inf                | Inf                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95808cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02945573, -0.02470975, -0.03183387,  0.02356944], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb02b3",
   "metadata": {},
   "source": [
    "This is a *box* environment - the actions and observations are arrays or vectors of numbers. These arrays are referred to as \"boxes\" because they contain numerical values in a specific format or range.. It's a space that contains numbers arranged in an array or vector, like the dimensions of a box.\n",
    "\n",
    "Or for the more mathematically minded:\n",
    "\n",
    "![box_env](box_env.png)\n",
    "\n",
    "### Rewards - Keep the pole upright for as long as possible, a reward of +1 for every step taken\n",
    "\n",
    "### Episode End\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "1. Termination: Pole Angle is greater than ¬±12¬∞\n",
    "2. Termination: Cart Position is greater than ¬±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "### More info\n",
    "\n",
    "- This is a fixed environment (as opposed to a continuous environment). 200 frames, not forever eg a game where you run out of lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c93007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:14.0\n",
      "Episode:2 Score:27.0\n",
      "Episode:3 Score:19.0\n",
      "Episode:4 Score:20.0\n",
      "Episode:5 Score:34.0\n"
     ]
    }
   ],
   "source": [
    "# let's see what it looks like when the model randomly takes actions\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes =  5 \n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset() # Initial set of observations\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() #only a 1 or a 0\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cc8ab",
   "metadata": {},
   "source": [
    "### Randomness won't get you nowhere, y'hear\n",
    "\n",
    "So - randomly taking actions is the baseline. There's no training going on here, we're just randomly picking something in the action space (here, that's just a one or a zero for left and right. So let's pick an algorithm and get training!\n",
    "\n",
    "### Algorithm Choice\n",
    "\n",
    "There's a ton of different algos out there to choose from. \n",
    "Two main types:\n",
    "\n",
    "Model-Free RL:\n",
    "Only uses the current state values to try to make a prediction. It focuses on learning from experience directly. Does not build an explicit understanding or representation of how the environment works.\n",
    "\n",
    "Model-Based RL:\n",
    "Prediction about the future state of the model to try to generate a best possible action. It involves learning and utilizing an explicit model of the environment. This model is used to predict how the environment will behave in the future based on different actions taken. Then, this prediction is used to make decisions about which actions are likely to lead to the best outcomes.\n",
    "\n",
    "| Name           | Box | Discrete | MultiDiscrete | MultiBinary | Multi Processing |\n",
    "|----------------|-----|----------|---------------|-------------|------------------|\n",
    "| ARS 1          | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| A2C            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| DDPG           | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| DQN            | ‚ùå   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| HER            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| PPO            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| QR-DQN 1       | ‚ùå   | Ô∏è ‚úîÔ∏è       | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| RecurrentPPO 1 | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| SAC            | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TD3            | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TQC 1          | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TRPO 1         | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| Maskable PPO 1 | ‚ùå   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "\n",
    "\n",
    "Which algo you can use is based on the action space, not so much the observation space. So, in our case, we have a discrete action space. We're going to use PPO.\n",
    "\n",
    "Because it was in the tutorial I watched. Sue me.\n",
    "\n",
    "It stands for for Proximal Policy Optimization\n",
    "\n",
    "Proximal - because it adjusts the policy gradually\n",
    "It tries to balance exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03307a2",
   "metadata": {},
   "source": [
    "### Pseudocode example\n",
    "```\n",
    "# Initialize policy network\n",
    "policy_network = initialize_policy_network()\n",
    "\n",
    "# Set hyperparameters\n",
    "# These are not learned from the data but is set before the learning process begins. \n",
    "\n",
    "num_episodes = 1000\n",
    "num_policy_updates = 10\n",
    "buffer_size = 10000\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Repeat for multiple episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize buffer to store experiences\n",
    "    buffer = []\n",
    "    \n",
    "    # Interact with the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Collect experiences by taking actions using the policy\n",
    "        action_probabilities = policy_network.predict(state)\n",
    "        action = sample_action_from_distribution(action_probabilities)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Store experience in the buffer\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Update policy using collected experiences\n",
    "    for update in range(num_policy_updates):\n",
    "        # Prepare data from the buffer for policy update\n",
    "        states, actions, rewards, next_states, dones = zip(*buffer)\n",
    "        \n",
    "        # Calculate advantages (using e.g., generalized advantage estimation)\n",
    "        advantages = calculate_advantages(rewards, dones, states, next_states, gamma)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Perform policy gradient updates (PPO clipping)\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            action = actions[i]\n",
    "            advantage = advantages[i]\n",
    "            \n",
    "            old_action_prob = policy_network.predict(state)\n",
    "            old_action_prob = old_action_prob[action]\n",
    "            \n",
    "            # Calculate new action probabilities\n",
    "            new_action_prob = policy_network.predict(state)\n",
    "            new_action_prob = new_action_prob[action]\n",
    "            \n",
    "            # Calculate surrogate objective (PPO objective)\n",
    "            ratio = new_action_prob / (old_action_prob + 1e-8)\n",
    "            clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            surrogate = clipped_ratio * advantage\n",
    "            \n",
    "            # Update policy by minimizing the surrogate objective\n",
    "            policy_network.train_step(state, surrogate, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb183aa",
   "metadata": {},
   "source": [
    "### Training Metrics\n",
    "\n",
    "\n",
    "Let's focus on two types:\n",
    "\n",
    "- Evaluation metrics\n",
    "  - all to with episode length and reward\n",
    "- Other metrics\n",
    "  - explained variance - how much of the variance in the environment our agent can explain\n",
    "  - learning rate - how fast our policy is updating\n",
    "  - how many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb98ac",
   "metadata": {},
   "source": [
    "### Cat break and refocus\n",
    "Here's some inspiration from Mowgli to get you back into it:\n",
    "\n",
    "![cat](cat.jpg)\n",
    "\n",
    "\n",
    "Wait! Don't go to sleep! or work on your project! Or watch TikTok!\n",
    "\n",
    "\n",
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6448b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7320f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:168\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs_tensor)\n\u001b[1;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/utils.py:483\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03mMoves the observation to the given device.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "820d3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole1')\n",
    "# model.save(PPO_Path)\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7712db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ce80c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:322.0\n",
      "Episode:2 Score:271.0\n",
      "Episode:3 Score:272.0\n",
      "Episode:4 Score:261.0\n",
      "Episode:5 Score:214.0\n"
     ]
    }
   ],
   "source": [
    "# let's see what it looks like when the model actually works\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes =  5 \n",
    "for episode in range(1, episodes+1):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action, next_state = model.predict(obs) # Now using model here\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806817fa",
   "metadata": {},
   "source": [
    "So that was pretty neat!\n",
    "\n",
    "We can use tensorboard to look at actual metrics for how the model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d937c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')\n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791a6b5",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "So now that we know what's going on - lets look at the code behind the pokemon game that was running when we started!\n",
    "\n",
    "It used PPO! It used OpenAI Gym!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
