{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74e8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install 'stable-baselines3[extra]'\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8e84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym # https://gymnasium.farama.org/\n",
    "\n",
    "from IPython import display\n",
    "from stable_baselines3 import PPO # first algorithm - https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # vectorise environments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # easier to test how the model is actually performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566731ab",
   "metadata": {},
   "source": [
    "Learnings so far: gym is now called gymnasium, make sure you use '' around `stable-baselines3[extra]` when using pip, CartPole-v0 is deprecated, returned 5 values not 4 (done is deprecated, truncated is added), render_mode is set in the gym.make method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c33e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca36349",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes =  5 # Think of an episode as one full game within the environment. Some environments have a fixed length. CartPole is 200 frames Others are continuous, eg, play until not more lives left\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    # Action Space - actions you can take in your environment\n",
    "    # Observation Space - what your observations look like, a partial view. Here we have a Box environment\n",
    "    \n",
    "    while not terminated or not truncated:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample() # It's a discrete two action space, 0 or 1 Discrete(2)\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ededd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c539d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8fb0c43",
   "metadata": {},
   "source": [
    "Action space = what you can do. Here's its discrete 2 - you can move the cart left or right (1 or 0)\n",
    "Observation space = what you can see. Here it's Box 4 - values representing the pole and the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d0546",
   "metadata": {},
   "source": [
    "Type of Algorithms - there's a ton\n",
    "\n",
    "Model free only uses current state values to make a prediction - model based tries to figure out a model of the environment\n",
    "\n",
    "\n",
    "Model vs Model Free: learning based on predicitons of next state/reward or real samples\n",
    "\n",
    "Stable baselines (what we are working with) focuses on Model Free \n",
    "\n",
    "Algorithm choice can be based on the action space - PPO works on all, but some for example TD3 only works on Box\n",
    "\n",
    "Treat these algorithms as commodities - good to know in detail how they are put together and/or how they work\n",
    "\n",
    "Training metrics - what you get back during training\n",
    "\n",
    "Evaluation - all to do with episode length and reward\n",
    "Time - fps, iterations, time_elapsed, timesteps\n",
    "Loss - \n",
    "and \n",
    "Other explained variance - variance in the env your agent can explain\n",
    "\n",
    "You need pytorch for GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04514729",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69168164",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35997b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1c5f0",
   "metadata": {},
   "source": [
    "el_len_mean = how long it lasted\n",
    "el_rew_mean = how much reward it got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e999f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes =  5 # Think of an episode as one full game within the environment. Some environments have a fixed length. CartPole is 200 frames Others are continuous, eg, play until not more lives left\n",
    "for episode in range(1, episodes+1):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    # Action Space - actions you can take in your environment\n",
    "    # Observation Space - what your observations look like, a partial view. Here we have a Box environment\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action, next_state = model.predict(obs) # Now using model here\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        if truncated:\n",
    "            break\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749794d9",
   "metadata": {},
   "source": [
    "7. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eaae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a40b11",
   "metadata": {},
   "source": [
    "Core metric to pay attention too is average reward. this is an indication of how well the model is going to perform in that environment using that reward function. Second is an average episode length (this is important in continuous environment). \n",
    "\n",
    "To get a better model:\n",
    "1. Train for Longer\n",
    "2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1fdc3",
   "metadata": {},
   "source": [
    "8. Adding Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcd457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603643db",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ea6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=10000, verbose=1, best_model_save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a35762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4071eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.4        |\n",
      "|    ep_rew_mean          | 28.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008685252 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00316     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.62        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.8        |\n",
      "|    ep_rew_mean          | 34.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009661476 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.0669      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 43.2       |\n",
      "|    ep_rew_mean          | 43.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 178        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00881797 |\n",
      "|    clip_fraction        | 0.0941     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.638     |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22.8       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 52.2       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=460.80 +/- 78.40\n",
      "Episode length: 460.80 +/- 78.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 461         |\n",
      "|    mean_reward          | 461         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008257891 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 64.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 460.80  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1600e09a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed73f1f",
   "metadata": {},
   "source": [
    "9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2db15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = dict(pi=[128,128,128,128], vf=[128,128,128,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790a1d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24063971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.4        |\n",
      "|    ep_rew_mean          | 28.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014556032 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.00475     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 42.7        |\n",
      "|    ep_rew_mean          | 42.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015775437 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.1         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 26.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 57.9         |\n",
      "|    ep_rew_mean          | 57.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0137371365 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0251      |\n",
      "|    value_loss           | 46.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=460.60 +/- 78.80\n",
      "Episode length: 460.60 +/- 78.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 461         |\n",
      "|    mean_reward          | 461         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009680556 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | 72.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 92.3       |\n",
      "|    ep_rew_mean          | 92.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 38         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 319        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00912589 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.549     |\n",
      "|    explained_variance   | 0.711      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.7        |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    value_loss           | 36.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | 110         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014344509 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.76        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 13.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 128          |\n",
      "|    ep_rew_mean          | 128          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 410          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043078847 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.11         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00541     |\n",
      "|    value_loss           | 14.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 146          |\n",
      "|    ep_rew_mean          | 146          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 454          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101155015 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.546       |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.64         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 5.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008428037 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 6.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x161fd4280>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd28fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
