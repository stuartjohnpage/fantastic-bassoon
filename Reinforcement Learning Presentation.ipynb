{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0a7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742038d",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822b3ba",
   "metadata": {},
   "source": [
    "#### Disclaimer\n",
    "\n",
    "### I am not an expert. I don't really know that much about reinforcement learning as applied to a machine learning context beyond a couple of books and some research. There are ~probably~ people in here who know more than me and I welcome any corrections and or critisim or questions.\n",
    "\n",
    "\n",
    "The aim of this presentation is for a *general overview* of reinforcement learning as it applies to machine learning. We're going to look at the history briefly, and then dive into an example.\n",
    "\n",
    "We are not going to be getting into any of the tricksy mathematics, so far as we can avoid it.\n",
    "\n",
    "Just because you don't understand exactly how these algorithms work, don't avoid touching the topic. It's cool and interesting, and apparently, ever more relevant..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3652b8",
   "metadata": {},
   "source": [
    "### What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning is an area of machine learning that involves:\n",
    "- **Policy**: The strategy an agent employs to determine its actions.üïπÔ∏è\n",
    "- **Value Function**: An estimation of future rewards given a state and action.üéØ\n",
    "- **Model of the Environment**: Optionally used to simulate outcomes of actions (not always used).üìñ\n",
    "\n",
    "A cross between neuroscience, behaviorist psychology, engineering and mathematics.\n",
    "\n",
    "\n",
    "It is not unsupervised learning (find the pattern) or supervised learning (you've trained on the pattern). Fundamentally, this is more about training the actions of an agent, first by trial and error, and then by figuring out how to get more of a good thing.\n",
    "\n",
    "It is also *not* an LLM - but it has been used as the final step in training them!\n",
    "\n",
    "### How Did It Start?\n",
    "\n",
    "Originating from early studies in behavioral psychology, reinforcement learning has evolved through:\n",
    "- Thorndike's Law of Effect, where behaviors followed by rewards become more likely.\n",
    "\n",
    "_When a modifiable connection between a situation and a response is made and is accompanied or followed by a satisfying state of affairs, that connection's strength is increased: When made and accompanied or followed by an annoying state of affairs its strength is decreased. The strengthening effect of satisfyingness (or the weakening effect of annoyingness) upon a bond varies with the closeness of the connection between it and the bond_\n",
    "\n",
    "\n",
    "### Operant conditioning\n",
    "\n",
    "Note: this is *operant* conditioning, not *classical* conditioning. Mowgli's voluntary behaviour is affected the consequences of his behaviour.\n",
    "\n",
    "\n",
    "Let's see my glamourous assistant Mowgli assisting me:\n",
    "\n",
    "![come](come.gif)\n",
    "![shake](shake.gif)\n",
    "![lie_down](lie_down.gif)\n",
    "![jump](jump.gif)\n",
    "\n",
    "We can connect this to our earlier statement. \n",
    "Mowgli's *policy* is a strategy that maximises getting treats.\n",
    "Mowgli's *rewards* are his treats.\n",
    "Mowgli's value function estimates that when he sees me do certain motions combined with hearing certain sound queues, he sits/lies down/comes/shakes because he *thinks* that that will lead to a reward\n",
    "\n",
    "\n",
    "### A note about shaping\n",
    "\n",
    "![pigeon](pigeon.jpeg)\n",
    "\n",
    "B.F. Skinner wanted to train pigeons to fly guided missiles during WWII - and he even got it working. How does someone go about training an animal to do increasingly complex behaviour?\n",
    "\n",
    "Through auto-shaping. Reward closer *approximations* of behaviour, until you get the desired behaviour.\n",
    "\n",
    "This is still fraught with challenges\n",
    "\n",
    "- how can you make sure the correct behaviour is trained?\n",
    "- how do you avoid exploration vs exploitation?\n",
    "\n",
    "\n",
    "### How can we apply this to machines?\n",
    "\n",
    "Turing talking about how he might train a network:\n",
    "\n",
    "_When a configuration is reached for which the action is undetermined, a random choice for the missing data is made and the appropriate entry is made in the description tentatively, and is applied. When a pain stimulus occurs all tentative entries are cancelled, and when a pleasure stimulus occurs they are all made permanent_\n",
    "\n",
    "- Skinner's operant conditioning, which introduced the use of consequences for shaping behavior. ‚ö°\n",
    "- Bellman's development of dynamic programming, setting the stage for modern reinforcement learning algorithms.\n",
    "- Sutton and Barto founding the field of reinforcement learning\n",
    "\n",
    "Andrew Barto and Richard Sutton: they began in 1972 and didn't stop publishing papers and researching for 45 years\n",
    "\n",
    "### Cross-discipline advances\n",
    "\n",
    "Advances in psychology were critical to early work in the field of machine learning (and indeed, in it's inception). But what goes around comes around; years of breaking down behavioural whys in models in machine learning was in turn critical to understanding how dopamine works (or in other words, how our value function works).\n",
    "\n",
    "What is Dopamine? \n",
    "\n",
    "Reward? Attention? Novelty? Surprise?\n",
    "\n",
    "It's more explained by expectation - or as they called it in machine learning: Temporal Difference Learning.\n",
    "\n",
    "\n",
    "### Temporal Difference Learning\n",
    "\n",
    "Learning guesses from a guess\n",
    "\n",
    "As you move forward toward an uncertain future, you maintain a king of \"running expectation\" of how promising things seem. \n",
    "\n",
    "In a chess game, it might be the odds you give yourself to win the game. In a video game, it might be how much progress you expect to make or how many points you expect to rack up in total. These guesses fluctuate over time, and in general they get more accurate they closer you are to whatever  it is you're trying to predict.\n",
    "\n",
    "As our expectation fluctuates, we get difference between our successive expectation, each of which is a learning opportunity. These are temporal differences. \n",
    "\n",
    "Algorithm called TD-lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dynamic programming link\n",
    "\n",
    "RL takes the key steps from DP, which are all about making decisions and improving them, and adapts them for situations where you don‚Äôt have all the information from the start. It‚Äôs all about learning the best moves by practicing and seeing the results, much like learning to play a video game better by playing it more and more.\n",
    "\n",
    "\n",
    "### Common Definitions in Reinforcement Learning\n",
    "\n",
    "#### Operant Conditioning vs. Classical Conditioning\n",
    "- **Operant Conditioning**: A form of learning where an individual's behavior is modified by its consequences, such as rewards and punishments.\n",
    "- **Classical Conditioning**: A learning process that occurs when two stimuli are repeatedly paired; a response that is at first elicited by the second stimulus is eventually elicited by the first stimulus alone.\n",
    "\n",
    "#### On-policy vs. Off-policy Learning\n",
    "- **On-policy Learning**: The strategy where the learning agent evaluates and improves the policy that it uses to make decisions.\n",
    "- **Off-policy Learning**: The strategy where the agent learns a potentially different policy from the behavior policy that is used for making decisions and collecting data.\n",
    "\n",
    "#### Common Reinforcement Learning Algorithms\n",
    "- **Proximal Policy Optimization (PPO)**: A policy gradient method for reinforcement learning that uses a clipped surrogate objective function to prevent large policy updates, thereby improving training stability.\n",
    "- **Deep Q-Network (DQN)**: An algorithm that combines Q-learning with deep neural networks to approximate the Q-value function, allowing the agent to learn from high-dimensional sensory input.\n",
    "\n",
    "### Additional Relevant Concepts\n",
    "\n",
    "#### Markov Decision Process (MDP)\n",
    "A mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "\n",
    "#### Temporal Difference (TD) Learning\n",
    "A class of model-free reinforcement learning methods that learn by bootstrapping from the estimated value of subsequent states, rather than waiting for a final outcome.\n",
    "\n",
    "#### Exploration vs. Exploitation\n",
    "A dilemma faced by learning agents about whether to explore new possibilities (exploration) or choose options that are known to yield high rewards (exploitation).\n",
    "\n",
    "#### Reward Function\n",
    "A function used to signal the success of an agent's actions. It's the key feedback signal that guides the learning algorithm in reinforcement learning.\n",
    "\n",
    "#### Œµ-greedy Policy\n",
    "A common policy for controlling the agent's balance between exploration and exploitation. With a certain probability Œµ, the agent explores a random action, and with probability 1-Œµ, it exploits the currently known best action.\n",
    "\n",
    "These definitions and concepts form the foundation of reinforcement learning theory and practice, providing a framework for developing algorithms that enable machines to learn and make decisions autonomously.\n",
    "\n",
    "\n",
    "#### Dynamic Programming\n",
    "This technique solves problems by breaking them into smaller, overlapping sub-problems. The results are then stored in a table to be reused so the same problem will not have to be computed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0495cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5702887\n"
     ]
    }
   ],
   "source": [
    "# Function for nth Fibonacci number (naive, not using dynamic programming)\n",
    " \n",
    "def fibonacci_basic(n):\n",
    "    if n<= 0:\n",
    "        print(\"Incorrect input\")\n",
    "    # First Fibonacci number is 0\n",
    "    elif n == 1:\n",
    "        return 0\n",
    "    # Second Fibonacci number is 1\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci_basic(n-1)+fibonacci_basic(n-2)\n",
    " \n",
    "print(fibonacci_basic(35))\n",
    "\n",
    "### Each number is just re-calculated every single time for every recursive call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457b8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7778742049\n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073, 4807526976, 7778742049]\n"
     ]
    }
   ],
   "source": [
    "# Function for nth fibonacci number - Dynamic Programming\n",
    " \n",
    "FibArray = [0, 1]\n",
    " \n",
    "def fibonacci_dynamic(n):\n",
    "    if n<0:\n",
    "        print(\"Incorrect input\")\n",
    "    elif n<= len(FibArray):\n",
    "        return FibArray[n-1]\n",
    "    else:\n",
    "        temp_fib = fibonacci_dynamic(n-1)+fibonacci_dynamic(n-2)\n",
    "        FibArray.append(temp_fib)\n",
    "        return temp_fib\n",
    " \n",
    "print(fibonacci_dynamic(50))\n",
    "print(FibArray)\n",
    "\n",
    "### Each number is calculated just once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b87a0",
   "metadata": {},
   "source": [
    "### Live Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd5ae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.tenor.com/IJuLSEYNCcAAAAAC/its-happening.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://media.tenor.com/IJuLSEYNCcAAAAAC/its-happening.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79a9017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://gymnasium.farama.org/_images/cart_pole.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bc542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.26.1)\n",
      "Requirement already satisfied: torch>=1.13 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.2)\n",
      "Requirement already satisfied: matplotlib in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.8.1)\n",
      "Requirement already satisfied: opencv-python in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.8.1.78)\n",
      "Requirement already satisfied: pygame in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: psutil in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.66.1)\n",
      "Requirement already satisfied: rich in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (13.6.0)\n",
      "Requirement already satisfied: shimmy~=1.1.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: pillow in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from stable-baselines3[extra]) (10.1.0)\n",
      "Requirement already satisfied: autorom~=0.6.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: click in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
      "Requirement already satisfied: requests in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.59.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (6.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
      "Requirement already satisfied: gymnasium in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'stable-baselines3[extra]'\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f2c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym # https://gymnasium.farama.org/\n",
    "import pygame\n",
    "\n",
    "from IPython import display\n",
    "from stable_baselines3 import PPO \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "from stable_baselines3.common.evaluation import evaluate_policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fb0e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8630ab",
   "metadata": {},
   "source": [
    "## Cartpole - training an agent to balance a pole on top of a cart.\n",
    "### Action Space: Defines the actions you can take in your environment. So, in our case, we have two actions in our action space:\n",
    " 0: Push cart to the left\n",
    " 1: Push cart to the right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3e3bf",
   "metadata": {},
   "source": [
    "### Observation Space: Defines what you can see:\n",
    "| Num | Observation           | Min                 | Max                |\n",
    "|-----|-----------------------|---------------------|--------------------|\n",
    "| 0   | Cart Position         | -4.8                | 4.8                |\n",
    "| 1   | Cart Velocity         | -Inf                | Inf                |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24¬∞) | ~ 0.418 rad (-24¬∞) |\n",
    "| 3   | Pole Angular Velocity | -Inf                | Inf                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95808cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.00712545, -0.02686494, -0.02453898,  0.04317247], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb02b3",
   "metadata": {},
   "source": [
    "This is a *box* environment - the actions and observations are arrays or vectors of numbers. These arrays are referred to as \"boxes\" because they contain numerical values in a specific format or range.. It's a space that contains numbers arranged in an array or vector, like the dimensions of a box.\n",
    "\n",
    "Or for the more mathematically minded:\n",
    "\n",
    "![box_env](box_env.png)\n",
    "\n",
    "### Rewards - Keep the pole upright for as long as possible, a reward of +1 for every step taken\n",
    "\n",
    "### Episode End\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "1. Termination: Pole Angle is greater than ¬±12¬∞\n",
    "2. Termination: Cart Position is greater than ¬±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "### More info\n",
    "\n",
    "- This is a fixed environment (as opposed to a continuous environment). 200 frames, not forever eg a game where you run out of lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c93007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:22.0\n",
      "Episode:2 Score:12.0\n",
      "Episode:3 Score:12.0\n",
      "Episode:4 Score:25.0\n",
      "Episode:5 Score:15.0\n"
     ]
    }
   ],
   "source": [
    "# let's see what it looks like when we randomly takes actions\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes =  5 \n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset() # Initial set of observations\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() #only a 1 or a 0\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cc8ab",
   "metadata": {},
   "source": [
    "### Randomness won't get you nowhere, y'hear\n",
    "\n",
    "So - randomly taking actions is the baseline. There's no training going on here, we're just randomly picking something in the action space (here, that's just a one or a zero for left and right. So let's pick an algorithm and get training!\n",
    "\n",
    "### Algorithm Choice\n",
    "\n",
    "There's a ton of different algos out there to choose from. \n",
    "Two main types:\n",
    "\n",
    "Model-Free RL:\n",
    "Only uses the current state values to try to make a prediction. It focuses on learning from experience directly. Does not build an explicit understanding or representation of how the environment works.\n",
    "\n",
    "Model-Based RL:\n",
    "Prediction about the future state of the model to try to generate a best possible action. It involves learning and utilizing an explicit model of the environment. This model is used to predict how the environment will behave in the future based on different actions taken. Then, this prediction is used to make decisions about which actions are likely to lead to the best outcomes.\n",
    "\n",
    "| Name           | Box | Discrete | MultiDiscrete | MultiBinary | Multi Processing |\n",
    "|----------------|-----|----------|---------------|-------------|------------------|\n",
    "| ARS 1          | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| A2C            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| DDPG           | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| DQN            | ‚ùå   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| HER            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| PPO            | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| QR-DQN 1       | ‚ùå   | Ô∏è ‚úîÔ∏è       | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| RecurrentPPO 1 | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| SAC            | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TD3            | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TQC 1          | ‚úîÔ∏è   | ‚ùå        | ‚ùå             | ‚ùå           | ‚úîÔ∏è                |\n",
    "| TRPO 1         | ‚úîÔ∏è   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "| Maskable PPO 1 | ‚ùå   | ‚úîÔ∏è        | ‚úîÔ∏è             | ‚úîÔ∏è           | ‚úîÔ∏è                |\n",
    "\n",
    "\n",
    "Which algo you can use is based on the action space, not so much the observation space. So, in our case, we have a discrete action space. We're going to use PPO.\n",
    "\n",
    "Because it was in the tutorial I watched. Sue me.\n",
    "\n",
    "It stands for for Proximal Policy Optimization\n",
    "\n",
    "Proximal - because it adjusts the policy gradually\n",
    "It tries to balance exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03307a2",
   "metadata": {},
   "source": [
    "### Pseudocode example of PPO\n",
    "```\n",
    "# Initialize policy network\n",
    "policy_network = initialize_policy_network()\n",
    "\n",
    "# Set hyperparameters\n",
    "# These are not learned from the data but is set before the learning process begins. \n",
    "\n",
    "num_episodes = 1000\n",
    "num_policy_updates = 10\n",
    "buffer_size = 10000\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Repeat for multiple episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize buffer to store experiences\n",
    "    buffer = []\n",
    "    \n",
    "    # Interact with the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Collect experiences by taking actions using the policy\n",
    "        action_probabilities = policy_network.predict(state)\n",
    "        action = sample_action_from_distribution(action_probabilities)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Store experience in the buffer\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Update policy using collected experiences\n",
    "    for update in range(num_policy_updates):\n",
    "        # Prepare data from the buffer for policy update\n",
    "        states, actions, rewards, next_states, dones = zip(*buffer)\n",
    "        \n",
    "        # Calculate advantages (using e.g., generalized advantage estimation)\n",
    "        advantages = calculate_advantages(rewards, dones, states, next_states, gamma)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Perform policy gradient updates (PPO clipping)\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            action = actions[i]\n",
    "            advantage = advantages[i]\n",
    "            \n",
    "            old_action_prob = policy_network.predict(state)\n",
    "            old_action_prob = old_action_prob[action]\n",
    "            \n",
    "            # Calculate new action probabilities\n",
    "            new_action_prob = policy_network.predict(state)\n",
    "            new_action_prob = new_action_prob[action]\n",
    "            \n",
    "            # Calculate surrogate objective (PPO objective)\n",
    "            ratio = new_action_prob / (old_action_prob + 1e-8)\n",
    "            clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            surrogate = clipped_ratio * advantage\n",
    "            \n",
    "            # Update policy by minimizing the surrogate objective\n",
    "            policy_network.train_step(state, surrogate, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb183aa",
   "metadata": {},
   "source": [
    "### Training Metrics\n",
    "\n",
    "\n",
    "Let's focus on two types:\n",
    "\n",
    "- Evaluation metrics\n",
    "  - all to with episode length and reward\n",
    "- Other metrics\n",
    "  - explained variance - how much of the variance in the environment our agent can explain\n",
    "  - learning rate - how fast our policy is updating\n",
    "  - how many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb98ac",
   "metadata": {},
   "source": [
    "### Cat break and refocus\n",
    "Here's some inspiration from Mowgli to get you back into it:\n",
    "\n",
    "![cat](cat.jpg)\n",
    "\n",
    "\n",
    "Wait! Don't go to sleep! or work on your project! Or watch TikTok!\n",
    "\n",
    "\n",
    "Let's train the model. And I can answer some questions while it's training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6448b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7320f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 45   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 44   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 89         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00966014 |\n",
      "|    clip_fraction        | 0.0822     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.687     |\n",
      "|    explained_variance   | -0.00749   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.99       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    value_loss           | 48.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008995921 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0773      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.1         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013387375 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 47          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    187\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820d3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole1')\n",
    "# model.save(PPO_Path)\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=2, render=True)\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8ce80c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stuart/opt/miniconda3/envs/rl-learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:223.0\n",
      "Episode:2 Score:256.0\n",
      "Episode:3 Score:330.0\n",
      "Episode:4 Score:240.0\n",
      "Episode:5 Score:246.0\n"
     ]
    }
   ],
   "source": [
    "# let's see what it looks like when the model actually works\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes =  5 \n",
    "for episode in range(1, episodes+1):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False \n",
    "    truncated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action, next_state = model.predict(obs) # Now using model here\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "pygame.display.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806817fa",
   "metadata": {},
   "source": [
    "So that was pretty neat!\n",
    "\n",
    "We can use tensorboard to look at actual metrics for how the model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d937c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')\n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791a6b5",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "\n",
    "Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
